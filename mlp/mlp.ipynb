{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import * # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1337)\n",
    "random.seed(1337)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from micrograd.engine import Value\n",
    "from micrograd.nn import Neuron, Layer, MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(MLP of [Layer of [ReLUNeuron(3)], Layer of [LinearNeuron(1)]],\n",
       " MLP of [Layer of [ReLUNeuron(3), ReLUNeuron(3)], Layer of [LinearNeuron(2)]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n = MLP(3, [1, 1])\n",
    "m = MLP(3, [2, 1])\n",
    "n, m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class _: # MLP\n",
    "    from typing import List\n",
    "    def __init__(self, nin: int, nouts: List[int]):\n",
    "        sz = [nin] + nouts\n",
    "        self.layers = [\n",
    "            Layer(sz[i], sz[i + 1], nonlin=i != len(nouts) - 1)\n",
    "            for i in range(len(nouts))\n",
    "        ]\n",
    "# MLP CONSTRUCTOR\n",
    "# this is straightforward right, we make layers of\n",
    "# cin, cout combos but we only make #cout of them\n",
    "# each layers takes nin starting with nin, but then\n",
    "# all other layers take the nouts[i] of the previous\n",
    "\n",
    "# LINEAR OUTPUT\n",
    "# the last layers is linear so we can predict any number not just [0,int)\n",
    "\n",
    "# RELU LAYERS\n",
    "# ReLU (Rectified Linear Unit) introduces non-linearity into neural networks\n",
    "# due to its non-linear nature. The ReLU function is defined as:\n",
    "#\n",
    "# f(x) = max(0, x)\n",
    "#\n",
    "# without relus (or any non linear activation) nn's would be linear funtions\n",
    "\n",
    "#  By definition, the ReLU is 𝑚𝑎𝑥(0,𝑥). Therefore, if we split the domain from\n",
    "# (−∞,0] or [0,∞), then the function is linear. However, it's easy to see\n",
    "# that 𝑓(−1)+𝑓(1)≠𝑓(0). Hence, by definition, ReLU is not linear. \n",
    "# https://datascience.stackexchange.com/a/26481\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def internals(n: MLP) -> None:\n",
    "  for layer in n.layers:\n",
    "    print(layer, '---')\n",
    "    for neuron in layer.neurons:\n",
    "      print(neuron, '*')\n",
    "      for value in neuron.parameters(): # .w and [.b]\n",
    "        print(value, '.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer of [ReLUNeuron(3)] ---\n",
      "ReLUNeuron(3) *\n",
      "Value(data=0.23550571390294128, grad=0) .\n",
      "Value(data=0.06653114721000164, grad=0) .\n",
      "Value(data=-0.26830328150124894, grad=0) .\n",
      "Value(data=0, grad=0) .\n",
      "Layer of [LinearNeuron(1)] ---\n",
      "LinearNeuron(1) *\n",
      "Value(data=0.1715747078045431, grad=0) .\n",
      "Value(data=0, grad=0) .\n"
     ]
    }
   ],
   "source": [
    "internals(n)\n",
    "# MLP Diagram:\n",
    "#\n",
    "#   Input (3 features)\n",
    "#       ↓\n",
    "# Layer 1 (ReLUNeuron) # w: [0.2, 0.1, 0.3], b: 0.0\n",
    "#       ↓\n",
    "# Layer 2 (LinearNeuron) # w: [0.2], b: 0.0\n",
    "#       ↓\n",
    "#  Output (1 output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer of [ReLUNeuron(3), ReLUNeuron(3)] ---\n",
      "ReLUNeuron(3) *\n",
      "Value(data=-0.6686254326224383, grad=0) .\n",
      "Value(data=0.6487474938152629, grad=0) .\n",
      "Value(data=-0.23259038277158273, grad=0) .\n",
      "Value(data=0, grad=0) .\n",
      "ReLUNeuron(3) *\n",
      "Value(data=0.5792256498313748, grad=0) .\n",
      "Value(data=0.8434530197925192, grad=0) .\n",
      "Value(data=-0.3847332240409951, grad=0) .\n",
      "Value(data=0, grad=0) .\n",
      "Layer of [LinearNeuron(2)] ---\n",
      "LinearNeuron(2) *\n",
      "Value(data=0.9844941451716409, grad=0) .\n",
      "Value(data=-0.5901079958448365, grad=0) .\n",
      "Value(data=0, grad=0) .\n"
     ]
    }
   ],
   "source": [
    "internals(m)\n",
    "# here's why this makes sense\n",
    "# you take three inputs, every neuron needs 3 weights plus a bias\n",
    "# but the number outputs of the first layer is two\n",
    "# so you need two neurons in the first layer\n",
    "# the linear layer gives two inputs and thus has two weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs = [\n",
    "    [2.0, 3.0, -1.0],\n",
    "    [3.0, -1.0, 0.5],\n",
    "    [0.5, 1.0, 1.0],\n",
    "    [1.0, 1.0, -1.0]\n",
    "]\n",
    "ys = [1.0, -1.0, -1.0, 1.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 3.8786498946619115 ['0.23', '0.05', '0.12', '0.03']\n",
      "2 10.819478474781187 ['3.24', '0.64', '0.75', '1.28']\n",
      "3 13.24678282646478 ['-1.81', '-0.41', '-0.70', '-1.22']\n",
      "4 4.0706682623564365 ['0.37', '0.55', '-0.13', '0.29']\n",
      "5 3.4335305562849223 ['0.13', '0.10', '-0.28', '0.03']\n",
      "6 3.0448729174651343 ['0.32', '0.16', '-0.28', '0.16']\n",
      "7 2.5598893769509488 ['0.50', '0.13', '-0.32', '0.24']\n",
      "8 1.971783060797325 ['0.71', '0.03', '-0.39', '0.33']\n",
      "9 1.3525657579825372 ['0.92', '-0.15', '-0.48', '0.41']\n",
      "10 0.8628703857312423 ['1.09', '-0.39', '-0.56', '0.46']\n",
      "11 0.5860348252925198 ['1.22', '-0.62', '-0.62', '0.51']\n",
      "12 0.525951331836336 ['1.30', '-0.64', '-0.67', '0.56']\n",
      "13 0.5031139379262789 ['1.09', '-0.72', '-0.72', '0.42']\n",
      "14 0.532468770083034 ['1.55', '-0.73', '-0.73', '0.72']\n",
      "15 0.8385808483139425 ['0.69', '-0.81', '-0.81', '0.18']\n",
      "16 1.4658746020062692 ['2.12', '-0.68', '-0.69', '1.13']\n",
      "17 4.674142084973772 ['-0.47', '-0.92', '-0.92', '-0.58']\n",
      "18 0.49716071302560283 ['1.08', '-0.63', '-0.60', '0.55']\n",
      "19 0.4294331795262642 ['1.34', '-0.67', '-0.64', '0.72']\n",
      "20 0.44047491744674255 ['0.93', '-0.75', '-0.74', '0.45']\n",
      "21 0.5872223280314004 ['1.65', '-0.74', '-0.71', '0.94']\n",
      "22 1.4692466039615848 ['0.30', '-0.85', '-0.85', '0.03']\n",
      "23 1.915505040213961 ['2.25', '-0.71', '-0.65', '1.37']\n",
      "24 5.761863611882342 ['-0.71', '-0.94', '-0.94', '-0.69']\n",
      "25 1.3473237775607687 ['0.38', '-0.61', '-0.59', '0.20']\n",
      "26 0.8855008653003422 ['1.61', '-0.53', '-0.46', '1.07']\n",
      "27 1.528989428495557 ['0.26', '-0.72', '-0.72', '0.09']\n",
      "28 1.164040655393548 ['1.87', '-0.61', '-0.55', '1.23']\n",
      "29 2.9437137715023676 ['-0.17', '-0.80', '-0.80', '-0.22']\n",
      "30 0.6363317471538809 ['1.52', '-0.60', '-0.54', '1.03']\n",
      "31 1.0821151771842112 ['0.41', '-0.74', '-0.74', '0.23']\n",
      "32 1.3005424612561243 ['1.95', '-0.66', '-0.58', '1.33']\n",
      "33 3.759042956999304 ['-0.37', '-0.86', '-0.86', '-0.36']\n",
      "34 0.4199033395685384 ['1.23', '-0.62', '-0.55', '0.88']\n",
      "35 0.3755660623734368 ['0.85', '-0.71', '-0.68', '0.59']\n",
      "36 0.45549055403720584 ['1.51', '-0.72', '-0.67', '1.06']\n",
      "37 1.119242101390244 ['0.34', '-0.83', '-0.83', '0.21']\n",
      "38 1.8323244539316954 ['2.15', '-0.66', '-0.64', '1.52']\n",
      "39 6.092404651499735 ['-0.80', '-0.96', '-0.96', '-0.69']\n",
      "40 1.4682731736250312 ['0.27', '-0.62', '-0.60', '0.21']\n",
      "41 0.8209168061317585 ['1.55', '-0.54', '-0.47', '1.16']\n",
      "42 1.393656243374092 ['0.25', '-0.71', '-0.71', '0.18']\n",
      "43 1.1316831306543353 ['1.81', '-0.61', '-0.53', '1.33']\n",
      "44 2.9494095427868077 ['-0.21', '-0.81', '-0.81', '-0.19']\n",
      "45 0.5994705377626038 ['1.47', '-0.61', '-0.54', '1.11']\n",
      "46 1.0673600339943312 ['0.36', '-0.75', '-0.75', '0.27']\n",
      "47 1.2595471255364572 ['1.90', '-0.67', '-0.59', '1.41']\n",
      "48 3.8249229692001503 ['-0.41', '-0.87', '-0.87', '-0.34']\n",
      "49 0.3739872675667209 ['1.19', '-0.62', '-0.56', '0.93']\n",
      "50 0.3281561372337238 ['0.83', '-0.72', '-0.69', '0.65']\n"
     ]
    }
   ],
   "source": [
    "nn = MLP(3, [4, 4, 1]) # i think this ignores our random seed sadly\n",
    "for _ in range(50):\n",
    "  ypred = [nn(x) for x in xs]\n",
    "  loss = sum((a - b)**2 for a, b in zip(ypred, ys))\n",
    "  for p in nn.parameters():\n",
    "      p.grad = 0.0\n",
    "  loss.backward()\n",
    "  for p in nn.parameters():\n",
    "      p.data += -0.05 * p.grad\n",
    "  print(_+1, loss.data, [f\"{x.data:1.2f}\" for x in ypred])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from cudagrad import tensor, Tensor\n",
    "import cudagrad \n",
    "\n",
    "class Module:\n",
    "\n",
    "    def zero_grad(self):\n",
    "        for p in self.parameters():\n",
    "            p.grad = 0\n",
    "\n",
    "    def parameters(self):\n",
    "        return []\n",
    "\n",
    "class Neuron(Module):\n",
    "    def __init__(self, nin, nonlin=True):\n",
    "        self.w = [tensor([1], [random.uniform(-1,1)]) for _ in range(nin)]\n",
    "        self.b = tensor([1], [0])\n",
    "        self.nonlin = nonlin\n",
    "\n",
    "    def __call__(self, x):\n",
    "        ans = tensor([1], [0])\n",
    "        for elem_x in x:\n",
    "            for elem_w in self.w:\n",
    "                if type(elem_x) != Tensor:\n",
    "                    elem_x = tensor([1], [elem_x])\n",
    "                ans = ans + (elem_w * elem_x)\n",
    "        ans = ans + self.b\n",
    "        return ans\n",
    "        # act = sum((wi*xi for wi,xi in zip(self.w, x)), self.b)\n",
    "        # return act.relu() if self.nonlin else act\n",
    "\n",
    "    def parameters(self):\n",
    "        return self.w + [self.b]\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"{'ReLU' if self.nonlin else 'Linear'}Neuron({len(self.w)})\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.09220615029335022]\n",
      "[0.0012150630354881287]\n"
     ]
    }
   ],
   "source": [
    "print(Neuron(2)([0.5, 0.2]).data)\n",
    "print(Neuron(2)([tensor([1], [0.5]), tensor([1], [0.2])]).data)\n",
    "# Layer(1, 2)\n",
    "# MLP(2, [2, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer(Module):\n",
    "\n",
    "    def __init__(self, nin, nout, **kwargs):\n",
    "        self.neurons = [Neuron(nin, **kwargs) for _ in range(nout)]\n",
    "\n",
    "    def __call__(self, x):\n",
    "        out = [n(x) for n in self.neurons]\n",
    "        return out[0] if len(out) == 1 else out\n",
    "\n",
    "    def parameters(self):\n",
    "        return [p for n in self.neurons for p in n.parameters()]\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"Layer of [{', '.join(str(n) for n in self.neurons)}]\"\n",
    "\n",
    "class MLP(Module):\n",
    "\n",
    "    def __init__(self, nin, nouts):\n",
    "        sz = [nin] + nouts\n",
    "        self.layers = [Layer(sz[i], sz[i+1], nonlin=i!=len(nouts)-1) for i in range(len(nouts))]\n",
    "\n",
    "    def __call__(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "\n",
    "    def parameters(self):\n",
    "        return [p for layer in self.layers for p in layer.parameters()]\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"MLP of [{', '.join(str(layer) for layer in self.layers)}]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__mul__(): incompatible function arguments. The following argument types are supported:\n    1. (self: cudagrad.Tensor, arg0: cudagrad.Tensor) -> cudagrad.Tensor\n\nInvoked with: <cudagrad.Tensor object at 0x112e36b70>, 2.0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m nn \u001b[39m=\u001b[39m MLP(\u001b[39m3\u001b[39m, [\u001b[39m4\u001b[39m, \u001b[39m4\u001b[39m, \u001b[39m1\u001b[39m]) \u001b[39m# i think this ignores our random seed sadly\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m50\u001b[39m):\n\u001b[0;32m----> 3\u001b[0m   ypred \u001b[39m=\u001b[39m [nn(x) \u001b[39mfor\u001b[39;49;00m x \u001b[39min\u001b[39;49;00m xs]\n\u001b[1;32m      4\u001b[0m   loss \u001b[39m=\u001b[39m \u001b[39msum\u001b[39m((a \u001b[39m-\u001b[39m b)\u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39m2\u001b[39m \u001b[39mfor\u001b[39;00m a, b \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(ypred, ys))\n\u001b[1;32m      5\u001b[0m   \u001b[39mfor\u001b[39;00m p \u001b[39min\u001b[39;00m nn\u001b[39m.\u001b[39mparameters():\n",
      "Cell \u001b[0;32mIn[15], line 3\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      1\u001b[0m nn \u001b[39m=\u001b[39m MLP(\u001b[39m3\u001b[39m, [\u001b[39m4\u001b[39m, \u001b[39m4\u001b[39m, \u001b[39m1\u001b[39m]) \u001b[39m# i think this ignores our random seed sadly\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m50\u001b[39m):\n\u001b[0;32m----> 3\u001b[0m   ypred \u001b[39m=\u001b[39m [nn(x) \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m xs]\n\u001b[1;32m      4\u001b[0m   loss \u001b[39m=\u001b[39m \u001b[39msum\u001b[39m((a \u001b[39m-\u001b[39m b)\u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39m2\u001b[39m \u001b[39mfor\u001b[39;00m a, b \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(ypred, ys))\n\u001b[1;32m      5\u001b[0m   \u001b[39mfor\u001b[39;00m p \u001b[39min\u001b[39;00m nn\u001b[39m.\u001b[39mparameters():\n",
      "Cell \u001b[0;32mIn[14], line 55\u001b[0m, in \u001b[0;36mMLP.__call__\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[1;32m     54\u001b[0m     \u001b[39mfor\u001b[39;00m layer \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayers:\n\u001b[0;32m---> 55\u001b[0m         x \u001b[39m=\u001b[39m layer(x)\n\u001b[1;32m     56\u001b[0m     \u001b[39mreturn\u001b[39;00m x\n",
      "Cell \u001b[0;32mIn[14], line 38\u001b[0m, in \u001b[0;36mLayer.__call__\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[0;32m---> 38\u001b[0m     out \u001b[39m=\u001b[39m [n(x) \u001b[39mfor\u001b[39;49;00m n \u001b[39min\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mneurons]\n\u001b[1;32m     39\u001b[0m     \u001b[39mreturn\u001b[39;00m out[\u001b[39m0\u001b[39m] \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(out) \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m \u001b[39melse\u001b[39;00m out\n",
      "Cell \u001b[0;32mIn[14], line 38\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[0;32m---> 38\u001b[0m     out \u001b[39m=\u001b[39m [n(x) \u001b[39mfor\u001b[39;00m n \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mneurons]\n\u001b[1;32m     39\u001b[0m     \u001b[39mreturn\u001b[39;00m out[\u001b[39m0\u001b[39m] \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(out) \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m \u001b[39melse\u001b[39;00m out\n",
      "Cell \u001b[0;32mIn[14], line 23\u001b[0m, in \u001b[0;36mNeuron.__call__\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[0;32m---> 23\u001b[0m     act \u001b[39m=\u001b[39m \u001b[39msum\u001b[39;49m((wi\u001b[39m*\u001b[39;49mxi \u001b[39mfor\u001b[39;49;00m wi,xi \u001b[39min\u001b[39;49;00m \u001b[39mzip\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mw, x)), \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mb)\n\u001b[1;32m     24\u001b[0m     \u001b[39mreturn\u001b[39;00m act\u001b[39m.\u001b[39mrelu() \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnonlin \u001b[39melse\u001b[39;00m act\n",
      "Cell \u001b[0;32mIn[14], line 23\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[0;32m---> 23\u001b[0m     act \u001b[39m=\u001b[39m \u001b[39msum\u001b[39m((wi\u001b[39m*\u001b[39;49mxi \u001b[39mfor\u001b[39;00m wi,xi \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mw, x)), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mb)\n\u001b[1;32m     24\u001b[0m     \u001b[39mreturn\u001b[39;00m act\u001b[39m.\u001b[39mrelu() \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnonlin \u001b[39melse\u001b[39;00m act\n",
      "\u001b[0;31mTypeError\u001b[0m: __mul__(): incompatible function arguments. The following argument types are supported:\n    1. (self: cudagrad.Tensor, arg0: cudagrad.Tensor) -> cudagrad.Tensor\n\nInvoked with: <cudagrad.Tensor object at 0x112e36b70>, 2.0"
     ]
    }
   ],
   "source": [
    "nn = MLP(3, [4, 4, 1]) # i think this ignores our random seed sadly\n",
    "for _ in range(50):\n",
    "  ypred = [nn(x) for x in xs]\n",
    "  loss = sum((a - b)**2 for a, b in zip(ypred, ys))\n",
    "  for p in nn.parameters():\n",
    "      p.grad = 0.0\n",
    "  loss.backward()\n",
    "  for p in nn.parameters():\n",
    "      p.data += -0.05 * p.grad\n",
    "  print(_+1, loss.data, [f\"{x.data:1.2f}\" for x in ypred])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
